{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end RAG Pipeline using LangChain, Chromadb, and Open-source LLMs (running locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook combines the major steps to creating a `RAG` pipeline using `LangChain` as taught in the following set of notebooks in this repo: \n",
    "- [`L1-Document_loading.ipynb`](L1-Document_loading.ipynb)\n",
    "- [`L2-Document_spilling.ipynb`](L2-Document_splitting.ipynb)\n",
    "- [`L3-Vectorstores_and_Embeddings.ipynb`](L3-Vectorstores_and_Embeddings.ipynb)\n",
    "- [`L4-Retrieval.ipynb`](L4-Retrieval.ipynb)\n",
    "- [`L5-Question_answering.ipynb`](L5-Question_answering.ipynb)\n",
    "\n",
    "The original tutorial uses `OpenAI` API for generating embeddings and response. The current notebook replaces `OpenAI` API with freely available alternative models. The list of `python` libraries used in this notebook is in the [`requirements.txt`](requirements.txt) file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Documents - PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each page is a Document.\n",
    "\n",
    "A Document contains text (page_content) and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generating 'Vector' Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed by Chromadb - in-memory 'vector' store\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Set the persist directory for Chroma\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "# Use HuggingFaceEmbeddings instead of OpenAIEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name='thenlper/gte-small'\n",
    ")\n",
    "\n",
    "# Initialize the Chroma vector store with the HuggingFace embedding model\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "# docs = vectordb.similarity_search(question,k=3)\n",
    "docs = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:\n",
      "middle of class, but because there won't be video you can safely sit there and make faces \n",
      "at me, and that won't show, okay?  \n",
      "Let's see. I also handed out this — ther e were two handouts I hope most of you have, \n",
      "course information handout. So let me just say a few words about parts of these. On the \n",
      "third page, there's a section that says Online Resources.  \n",
      "Oh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \n",
      "Testing, testing. Okay, cool. Thanks.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Document 1:\n",
      "Student:[Inaudible]?  \n",
      "Instructor (Andrew Ng):Yeah, I threw a lot of notations at you today. So M is the \n",
      "number of training examples and the number of training examples runs from one through \n",
      "M, and then is the feature vector that runs from zero through N. Does that make sense?  \n",
      "So this is the sum from one through M. It's sort of theta transpose X that's equal to sum \n",
      "from J equals zero through N of theta J, X, J. Does that make sense? It's the feature \n",
      "vectors that index from zero through N w\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Document 2:\n",
      "when you had a Q’s tow. Like you make it too small in your –  \n",
      "Instructor (Andrew Ng):Yes, absolutely. Yes. So locally weighted regression can run \n",
      "into – locally weighted regression is not a penancier for the problem of overfitting or \n",
      "underfitting. You can still run into the same problems with locally weighted regression. \n",
      "What you just said about – and so some of these things I’ll leave you to discover for \n",
      "yourself in the homework problem. You’ll actually see what you just mentioned. Yeah?  \n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming your list of Document objects is assigned to the variable 'documents'\n",
    "for index, document in enumerate(docs):\n",
    "    print(f\"Document {index}:\")\n",
    "    print(document.page_content[:500])\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator between documents for clarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory and file paths\n",
    "model_dir = \"models/Phi-3-mini-4k-instruct\"\n",
    "local_file = os.path.join(model_dir, \"Phi-3-mini-4k-instruct-fp16.gguf\")\n",
    "url = \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(local_file):\n",
    "    print(\"File not found locally. Downloading...\")\n",
    "    os.system(f\"wget {url} -O {local_file}\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "# helpful ref: https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=local_file,\n",
    "    n_gpu_layers=-1,  # CPU-only processing\n",
    "    max_tokens=100,\n",
    "    temperature=0.3,\n",
    "    top_p = 0.1,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetrievalQA chain - without prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2896302/4094420968.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Linear regression and logistic regression\n",
      "- Support vector machines (SVM)\n",
      "- Neural networks\n",
      "- k-nearest neighbors (kNN)\n",
      "- Naive Bayes classifier\n",
      "- Decision trees\n",
      "- Random forests\n",
      "- Gradient boosting\n",
      "- Locally weighted regression\n",
      "- Principal component analysis (PCA)\n",
      "- t-distributed stochastic neighbor embedding (t-SNE)\n",
      "- Support vector machines (SVM)\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetrievalQA chain - with prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is probability a class topic?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Response: Yes, probability is a class topic as it relates to machine learning and data analysis.\n",
      "===\n",
      "Yes, probability is a fundamental concept in machine learning classes, particularly when discussing algorithms that involve randomness or uncertainty, such as those used for classification tasks like distinguishing between male and female faces. It's essential for understanding the behavior of these models under different conditions.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===\n",
      "This class covers Machine Learning, focusing on understanding algorithms and their applications through hands-on projects. Topics include supervised learning with examples like distinguishing male from female faces, as well as administrative aspects such as grading homework assignments.\n",
      "\n",
      "\n",
      "Explanation: The context provided discusses a lecture related to machine learning, mentioning the use of online resources and an example involving facial recognition technology. It also briefly touches on classroom management regarding volume control\n"
     ]
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
